{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from scipy.stats import randint\n",
    "\n",
    "import itertools\n",
    "\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.width = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"s3://iefp-unemployment/modelling/modelling.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intervention feature preparation\n",
    "\n",
    "interv_cols = [col for col in df.columns if \"i_\" in col]\n",
    "X = df[interv_cols].copy()\n",
    "\n",
    "# Strip col names\n",
    "X.columns = [col.replace(\"i_\", \"\") for col in X.columns]\n",
    "\n",
    "# Make df boolean\n",
    "X = (X.notna()).astype('int')\n",
    "\n",
    "# Filter for frequent interventions\n",
    "frequent_i = X.mean()[X.mean() > 0.01].index.tolist()\n",
    "X = X[frequent_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demographics prep\n",
    "\n",
    "dems = [\"d_age\", \"d_gender\", \"d_civil_status\", \"d_rsi\", \"d_desired_work_time\", \"d_desired_contract\",\n",
    "       \"d_school_qualification\", \"d_college_qualification\", \"d_disabled\", \"d_subsidy\", \"d_previous_job_sector\",\n",
    "       \"d_desired_job_sector\", \"d_previous_job_experience\"]\n",
    "\n",
    "X[dems] = df[dems]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with missing values\n",
    "# Leave None type as feature in college_qual\n",
    "# Fill NAs with 0 for school qualification!!!\n",
    "# Fill NAs with 0 for previous job experience\n",
    "\n",
    "X['d_school_qualification'] = X['d_school_qualification'].fillna(0)\n",
    "X['d_previous_job_experience'] = X['d_previous_job_experience'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporal features\n",
    "\n",
    "# Encode temporal variables as strings\n",
    "\n",
    "X[\"register_month\"] = df.register_date.dt.month.astype(str)\n",
    "X[\"register_year\"] = df.register_date.dt.year.astype(str)\n",
    "\n",
    "# Convert categorical to dummies \n",
    "\n",
    "X = pd.get_dummies(X, drop_first=True, dummy_na=True)\n",
    "\n",
    "display(X.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set output variable\n",
    "\n",
    "Y = df[\"success\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test/Train split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare Random Forest pipeline with scaling (for Age and job experience)\n",
    "\n",
    "rf__scale_pipeline = Pipeline([\n",
    "    ('scale', MinMaxScaler()),\n",
    "    ('rf', RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=0))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search\n",
    "\n",
    "big_param_grid = [\n",
    "        {'rf__n_estimators': [1,10,100,500,1000,2500],\n",
    "         'rf__max_depth': [3,5,10,20,50,100],\n",
    "         'rf__max_features': ['sqrt','log2'],\n",
    "         'rf__min_samples_split': [2,5,10,20],\n",
    "         'rf__n_jobs': [-1]\n",
    "        },\n",
    "    ]\n",
    "\n",
    "med_param_grid = [\n",
    "        {'rf__n_estimators': [1,10,100,1000],\n",
    "         'rf__max_depth': [1,5,10,20],\n",
    "         'rf__max_features': ['sqrt','log2'],\n",
    "         'rf__min_samples_split': [2,5,10],\n",
    "         'rf__n_jobs': [-1]\n",
    "        },\n",
    "    ]\n",
    "\n",
    "param_grid = [\n",
    "        {'rf__n_estimators': [500],\n",
    "         'rf__max_depth': [2, 3]\n",
    "        },\n",
    "    ]\n",
    "\n",
    "\n",
    "rf_grid_search = GridSearchCV(rf__scale_pipeline, big_param_grid, cv=3, refit=True)\n",
    "\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "rf_final_model = rf_grid_search.best_estimator_\n",
    "\n",
    "print(confusion_matrix(y_test, rf_final_model.predict(X_test)))\n",
    "\n",
    "print(accuracy_score(y_test, rf_final_model.predict(X_test)))\n",
    "\n",
    "f = open(\"results.txt\", \"a\")\n",
    "f.write(str(rf_final_model) + \"\\n\")\n",
    "f.write(str(confusion_matrix(y_test, rf_final_model.predict(X_test))) + \"\\n\")\n",
    "f.write(str(accuracy_score(y_test, rf_final_model.predict(X_test))) + \"\\n\")\n",
    "f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutations\n",
    "\n",
    "print(rf_final_model.predict(X_test.head(10)))\n",
    "print(y_test.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = [c for c in itertools.product([0, 1], repeat=16) if sum(c) <= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_df = pd.DataFrame(combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = X_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data_fixed = user_data.drop(user_data.iloc[:,0:16], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data_fixed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_fixed_df = pd.DataFrame(np.tile(user_data_fixed.values, len(combo_df.index)).reshape(-1,len(user_data_fixed.columns)), \n",
    "                   columns=user_data_fixed.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_fixed_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df = combo_df.join(user_fixed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = pd.DataFrame(rf_final_model.predict_proba(predict_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities.columns = [\"unsuccessful\", \"successful\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = predict_df.join(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.iloc[:,0:16].columns = ['i_job_search_techn',\n",
    " 'i_train_active_life',\n",
    " 'i_tutoring_in_individual_job_search',\n",
    " 'i_professional_internships',\n",
    " 'i_internship_job',\n",
    " 'i_employment-insertion_contract',\n",
    " 'i_employment_contract_insertion',\n",
    " 'i_information_job_eval_orientat',\n",
    " 'i_collective_orientation_session',\n",
    " 'i_tutoring_in_collective_job_search',\n",
    " 'i_efa_s3_type_a',\n",
    " 'i_modular',\n",
    " 'i_efa_n3_vocational_train',\n",
    " 'i_assertive_communication_job_search_techn',\n",
    " 'i_entrepreneurship_skills_job_search_techn',\n",
    " 'i_train_external_entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(results_df.sort_values(by=['successful'], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare LogReg pipeline\n",
    "\n",
    "logreg_pipeline = Pipeline([\n",
    "    ('logreg', LogisticRegression(penalty='l1', C=1e5)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search\n",
    "\n",
    "param_grid = [\n",
    "        {'logreg__penalty': ['l1','l2'],\n",
    "         'logreg__C': [0.00001,0.0001,0.001,0.01,0.1,1,10]\n",
    "        },\n",
    "    ]\n",
    "\n",
    "logreg_grid_search = GridSearchCV(logreg_pipeline, param_grid, cv=3, refit=True)\n",
    "\n",
    "logreg_grid_search.fit(X_train, y_train)\n",
    "\n",
    "logreg_final_model = logreg_grid_search.best_estimator_\n",
    "\n",
    "print(confusion_matrix(y_test, logreg_final_model.predict(X_test)))\n",
    "\n",
    "print(accuracy_score(y_test, logreg_final_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Gradient boost pipeline\n",
    "\n",
    "gboost_pipeline = Pipeline([\n",
    "    ('gboost', GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=10)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search\n",
    "\n",
    "param_grid = [\n",
    "        {'gboost__n_estimators': [1,10,100,1000,10000],\n",
    "         'gboost__learning_rate' : [0.001,0.01,0.05,0.1,0.5],\n",
    "         'gboost__subsample' : [0.1,0.5,1.0],\n",
    "         'gboost__max_depth': [1,3,5,10,20,50,100]\n",
    "        },\n",
    "    ]\n",
    "\n",
    "gboost_grid_search = GridSearchCV(gboost_pipeline, param_grid, cv=3, refit=True)\n",
    "\n",
    "gboost_grid_search.fit(X_train, y_train)\n",
    "\n",
    "gboost_final_model = gboost_grid_search.best_estimator_\n",
    "\n",
    "print(confusion_matrix(y_test, gboost_final_model.predict(X_test)))\n",
    "\n",
    "print(accuracy_score(y_test, gboost_final_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision / Recall graph\n",
    "\n",
    "\n",
    "def plot_precision_recall(y_test, y_pred, y_prob):\n",
    "    average_precision = average_precision_score(y_test, y_pred)\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "    print(\"{} Precision at {} Recall\".format(precision[recall>0.8].max(), 0.8))\n",
    "\n",
    "    step_kwargs = ({'step': 'post'})\n",
    "    plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "    plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(average_precision))\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_prob = pipeline.predict_proba(X_test)[:,1]\n",
    "plot_precision_recall(y_test, y_pred, y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roc curve\n",
    "\n",
    "fpr, tpr, threshold = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "\n",
    "pd.Series(rf.feature_importances_, X_train.columns).sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search script dump:\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from scipy.stats import randint\n",
    "\n",
    "import itertools\n",
    "\n",
    "# evaluation\n",
    "\n",
    "def evaluate(y_test, y_pred, y_prob):\n",
    "    results = \"\\n---\"\n",
    "    results = results + str(confusion_matrix(y_test, y_pred)) + \"\\n---\" + \"Accuracy: {:.2f}%\".format(100 * accuracy_score(y_test, y_pred)) + \"\\n---\"\n",
    "        \n",
    "    results = results + str(average_precision_score(y_test, y_pred))\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "    results = results + \"{:.2f}% Precision at {}% Recall\".format(100 * precision[recall>0.8].max(), 80) + \"\\n\"\n",
    "    results = results + \"Precision: \" + str(precision) + \"\\n\" + \"Recall: \" + str(recall)\n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"running...\")\n",
    "\n",
    "df = pd.read_parquet(\"s3://iefp-unemployment/modelling/modelling.parquet\")\n",
    "\n",
    "\n",
    "# modelling table construction\n",
    "\n",
    "def transform_interventions(df: pd.DataFrame, interventions: list):\n",
    "    df = df[interventions]\n",
    "    df = (df.notna()).astype('int')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def construct_table(df): \n",
    "    # Filter for frequent interventions\n",
    "    # WARNING: data leakage\n",
    "    # frequent_i = df_model.mean()[df_model.mean() > 0.01].index.tolist()\n",
    "    # df_model = df_model[frequent_i]\n",
    "    \n",
    "    \n",
    "    interv_cols = [col for col in df.columns if \"i_\" in col]\n",
    "    df_model = transform_interventions(df, interv_cols)\n",
    "    \n",
    "    dems = [\n",
    "    \"d_age\",\n",
    "    \"d_gender\",\n",
    "    \"d_civil_status\",\n",
    "    \"d_rsi\",\n",
    "    \"d_desired_work_time\",\n",
    "    \"d_desired_contract\",\n",
    "    \"d_disabled\",\n",
    "    \"d_nationality\",\n",
    "    \"d_desired_job_sector\",\n",
    "    \"d_previous_job_sector\",\n",
    "    \"d_school_qualification\", \n",
    "    \"d_college_qualification\",\n",
    "    \"d_subsidy\",\n",
    "    \"d_previous_job_experience\",\n",
    "    \"register_reason\",\n",
    "    \"d_parish\",\n",
    "    \"d_professional_training\"\n",
    "    ]\n",
    "    \n",
    "    df_model[dems] = df[dems]\n",
    "    \n",
    "    # Dealing with missing values\n",
    "    # Leave None type as feature in college_qual\n",
    "    # Fill NAs with 0 for school qualification!!!\n",
    "    # Fill NAs with 0 for previous job experience\n",
    "\n",
    "    df_model['d_school_qualification'] = df_model['d_school_qualification'].fillna(0)\n",
    "    df_model['d_previous_job_experience'] = df_model['d_previous_job_experience'].fillna(0)\n",
    "\n",
    "    # Seasonal features\n",
    "    # Encode temporal variables as strings to enable dummy var creation\n",
    "    df_model[\"register_month\"] = df.register_date.dt.month.astype(str)\n",
    "    df_model[\"register_year\"] = df.register_date.dt.year.astype(str)\n",
    "\n",
    "    \n",
    "    df_model = pd.get_dummies(df_model, drop_first=True, dummy_na=True)\n",
    "\n",
    "    # Output\n",
    "    df_model[\"ttj_sub_12\"] = (df[\"journey_length\"] < 365) & (df[\"success\"] == True)\n",
    "    \n",
    "    return df_model\n",
    "\n",
    "def run_grid_pipe(data, pipeline, gs_params, filename):\n",
    "    \n",
    "    # Get holdout set\n",
    "\n",
    "    data[\"exit_date\"] = df[\"exit_date\"]\n",
    "    df_hold_out = data[data[\"exit_date\"] >= \"2019-01-01\"].drop(\"exit_date\", axis=1)\n",
    "\n",
    "    # Get training set\n",
    "\n",
    "    data[\"exit_date\"] = df[\"exit_date\"]\n",
    "    df_train = data[data[\"exit_date\"] <= \"2019-01-01\"].drop(\"exit_date\", axis=1)\n",
    "\n",
    "    y = df_train[\"ttj_sub_12\"]\n",
    "    X = df_train.drop(\"ttj_sub_12\", axis=1)\n",
    "    \n",
    "    grid_search = GridSearchCV(pipeline, gs_params, cv=5, refit=True, scoring=\"average_precision\")\n",
    "\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    final_model = grid_search.best_estimator_\n",
    "\n",
    "    y_pred = final_model.predict(X)\n",
    "    y_prob = final_model.predict_proba(X)[:,1]\n",
    "\n",
    "    training_evaluation = evaluate(y, y_pred, y_prob)\n",
    "\n",
    "    # Train on all data and then test on holdout set\n",
    "\n",
    "    final_model.fit(X, y)\n",
    "\n",
    "    y_hold = df_hold_out[\"ttj_sub_12\"]\n",
    "    X_hold = df_hold_out.drop(\"ttj_sub_12\", axis=1)\n",
    "\n",
    "    y_test_pred = final_model.predict(X_hold)\n",
    "    y_test_prob = final_model.predict_proba(X_hold)[:,1]\n",
    "\n",
    "    test_evaluation = evaluate(y_hold, y_test_pred, y_test_prob)\n",
    "    \n",
    "\n",
    "    f = open(\"results.txt\", \"a\")\n",
    "    f.write(\"\\n\" + filename + \"\\n\" + str(final_model) + \"\\n\")\n",
    "    f.write(\"Evaluation of model on training data: \\n\")\n",
    "    f.write(training_evaluation)\n",
    "    f.write(\"Evaluation of model on test data: \\n\")\n",
    "    f.write(test_evaluation)\n",
    "    f.write(str(confusion_matrix(y_hold, final_model.predict(X_hold))) + \"\\n\")\n",
    "    f.write(str(accuracy_score(y_hold, final_model.predict(X_hold))) + \"\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "    # save the model to disk\n",
    "\n",
    "    pickle.dump(final_model, open(filename, 'wb'))\n",
    "\n",
    "\n",
    "##############################################\n",
    "\n",
    "df_model = construct_table(df)\n",
    "\n",
    "##############################################\n",
    "\n",
    "\n",
    "# Prepare Random Forest pipeline with scaling (for Age and job experience)\n",
    "\n",
    "rf__scale_pipeline = Pipeline([\n",
    "    ('scale', MinMaxScaler()),\n",
    "    ('rf', RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=0, class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "\n",
    "# RF Grid search\n",
    "\n",
    "big_param_grid = [\n",
    "        {'rf__n_estimators': [1,10,100,500,1000,2500],\n",
    "         'rf__max_depth': [3,5,10,20,50,100],\n",
    "         'rf__max_features': ['sqrt','log2'],\n",
    "         'rf__min_samples_split': [2,5,10,20],\n",
    "         'rf__n_jobs': [-1]\n",
    "        },\n",
    "    ]\n",
    "\n",
    "med_param_grid = [\n",
    "        {'rf__n_estimators': [1,10,100,1000],\n",
    "         'rf__max_depth': [1,5,10,20],\n",
    "         'rf__max_features': ['sqrt','log2'],\n",
    "         'rf__min_samples_split': [2,5,10],\n",
    "         'rf__n_jobs': [-1]\n",
    "        },\n",
    "    ]\n",
    "\n",
    "param_grid = [\n",
    "        {'rf__n_estimators': [500],\n",
    "         'rf__max_depth': [2, 3]\n",
    "        },\n",
    "    ]\n",
    "\n",
    "# Prepare LogReg pipeline\n",
    "\n",
    "logreg_pipeline = Pipeline([\n",
    "    ('scale', MinMaxScaler()),\n",
    "    ('logreg', LogisticRegression(penalty='l1', C=1e5, class_weight=\"balanced\", n_jobs=-1)),\n",
    "])\n",
    "\n",
    "# LR Grid search\n",
    "\n",
    "lg_param_grid = [\n",
    "        {'logreg__penalty': ['l1','l2'],\n",
    "         'logreg__C': [0.00001,0.0001,0.001, 0.01,0.1,1,10]\n",
    "        },\n",
    "    ]\n",
    "\n",
    "# Prepare Gradient boost pipeline\n",
    "\n",
    "gboost_pipeline = Pipeline([\n",
    "    ('scale', MinMaxScaler()),\n",
    "    ('gboost', GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=10)),\n",
    "])\n",
    "\n",
    "\n",
    "# Gradient Boosting Trees Grid search\n",
    "\n",
    "gb_param_grid = [\n",
    "        {'gboost__n_estimators': [1,10,100,1000,10000],\n",
    "         'gboost__learning_rate' : [0.001,0.01,0.05,0.1,0.5],\n",
    "         'gboost__subsample' : [0.1,0.5,1.0],\n",
    "         'gboost__max_depth': [1,3,5,10,20,50,100]\n",
    "        },\n",
    "    ]\n",
    "\n",
    "# Prepare adaboost pipeline\n",
    "\n",
    "adaboost_pipeline = Pipeline([\n",
    "    ('scale', MinMaxScaler()),\n",
    "    ('adaboost', AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), algorithm=\"SAMME\", n_estimators=200))\n",
    "])\n",
    "\n",
    "\n",
    "# Ada grid search\n",
    "\n",
    "ada_param_grid = [\n",
    "        {'algorithm': ['SAMME', 'SAMME.R'],\n",
    "         'n_estimators': [1,10,100,500,1000,10000]\n",
    "        },\n",
    "    ]\n",
    "\n",
    "# Prepare SVM pipeline\n",
    "\n",
    "svm_pipeline = Pipeline([\n",
    "    ('scale', MinMaxScaler()),\n",
    "    ('svm', svm.SVC(kernel='linear', probability=True, random_state=0, class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "svm_param_grid = [\n",
    "        {'C' :[0.00001,0.0001,0.001,0.01,0.1,1,10],\n",
    "         'gamma':[1,0.1,0.001,0.0001],\n",
    "         'kernel':['linear', 'rbf', 'polynomial', 'sigmoid']\n",
    "        },\n",
    "    ]\n",
    "\n",
    "# Run pipelines\n",
    "\n",
    "run_grid_pipe(df_model, rf__scale_pipeline, param_grid, \"rf_test.sav\")\n",
    "run_grid_pipe(df_model, logreg_pipeline, lg_param_grid, \"lg_test.sav\")\n",
    "run_grid_pipe(df_model, gboost_pipeline, gb_param_grid, \"gb_test.sav\")\n",
    "run_grid_pipe(df_model, adaboost_pipeline, ada_param_grid, \"ada_test.sav\")\n",
    "run_grid_pipe(df_model, svm_pipeline, svm_param_grid, \"svm_test.sav\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iefp",
   "language": "python",
   "name": "iefp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
