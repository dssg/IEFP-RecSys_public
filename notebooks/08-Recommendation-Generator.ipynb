{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pickle\n",
    "import boto3\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from scipy.stats import randint\n",
    "\n",
    "import itertools\n",
    "\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.width = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_parquet(\"s3://iefp-unemployment/copy_test_train/train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_parquet(\"s3://iefp-unemployment/copy_test_train/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train.loc[:, \"ttj_sub_12\"]\n",
    "X_train = df_train.drop([\"ttj\", \"ttj_sub_12\"], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "response = s3.get_object(Bucket='iefp-unemployment', Key='models/2019/08/07/random_forest_T175519.pkl')\n",
    "model = pickle.loads(response[\"Body\"].read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw permutations\n",
    "\n",
    "with open('../src/iefp/modelling/100k_combinations.pkl', 'rb') as f:\n",
    "   a = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_permutation_df(raw_permutations, zero_columns, max_interventions):\n",
    "    # Strip binary prefix\n",
    "    bin_string = [bin[2:] for bin in raw_permutations]\n",
    "    \n",
    "    # Convert strings to floats\n",
    "    lst_bin_floats = [np.array(list(map(float, bin))) for bin in bin_string]\n",
    "    \n",
    "    # Limit max number of interventions per journey\n",
    "    limited_combos = [com for com in lst_bin_floats if sum(com) <= max_interventions]\n",
    "    \n",
    "    # Convert list of binary numbers to dataframe, fill NAs\n",
    "    permutations =  pd.DataFrame(limited_combos).fillna(0.0)\n",
    "    \n",
    "    # Add zero columns if needed\n",
    "    if zero_columns:\n",
    "        height = len(permutations.index)\n",
    "        df_0 = pd.DataFrame(0, index=range(height), columns=range(zero_columns)).astype(float)\n",
    "        permutations = pd.concat([permutations, df_0], axis=1, ignore_index=True)\n",
    "    \n",
    "    return permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_recommendations(journey_series, index, model, permutations, number_results):\n",
    "    # Create user dataframe to match permutation size\n",
    "    journey_fixed_df = pd.DataFrame(np.tile(journey_series.values, len(permutations.index)).reshape(-1,len(journey_series.index)), \n",
    "                       columns=journey_series.index)\n",
    "    \n",
    "    # Strip interventions and output variable from user dataframe\n",
    "    journey_fixed_df = journey_fixed_df.drop([col for col in journey_fixed_df.columns if \"i_\" in col] +\n",
    "                                             [\"ttj\", \"ttj_sub_12\"], axis='columns')\n",
    "    \n",
    "    # Join demographic features with permutation dataframe\n",
    "    predict_df = journey_fixed_df.join(permutations)\n",
    "    \n",
    "    # Predict for all permutations\n",
    "    probabilities = pd.DataFrame(model.predict_proba(predict_df))\n",
    "    probabilities.columns = [\"unsuccessful\", \"successful\"]\n",
    "\n",
    "    #Create final top_n dataframe\n",
    "    results_df = predict_df.join(probabilities)\n",
    "    \n",
    "    top_n = results_df.sort_values(by=['successful'], ascending=False).iloc[0:number_results,:]\n",
    "    top_n[\"journey_id\"] = index\n",
    "\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_top_recommendations_table(df, model, permutations, number_results):\n",
    "    # Prepare empty dataframe\n",
    "    recommendations_df = pd.DataFrame()\n",
    "    \n",
    "    # Iterate through each user and append the top 5 recommendations to an empty dataframe\n",
    "    for index, journey in df.iterrows():\n",
    "        user_recommendations = get_user_recommendations(journey, index, model, permutations, number_results)\n",
    "        recommendations_df = pd.concat([recommendations_df,user_recommendations])\n",
    "\n",
    "    return recommendations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_test = df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "top_5_10 = gen_top_recommendations_table(small_test, model, generate_permutation_df(a, 20, 3), 20)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_10.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = top_5_10.groupby(\"journey_id\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iefp",
   "language": "python",
   "name": "iefp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
