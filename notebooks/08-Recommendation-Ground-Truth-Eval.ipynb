{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pickle\n",
    "import boto3\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from scipy.stats import randint\n",
    "\n",
    "import itertools\n",
    "\n",
    "from iefp.recommendation import *\n",
    "\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.width = 0\n",
    "\n",
    "from iefp.data.postgres import *\n",
    "from iefp.data.constants import *\n",
    "results = query_db(\"SELECT * FROM {} LIMIT 5;\".format(Database.EVALUATION_TABLE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_parquet(\"s3://iefp-unemployment/copy_test_train/train_T180509.parquet\")\n",
    "df_test = pd.read_parquet(\"s3://iefp-unemployment/copy_test_train/test_T180509.parquet\")\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "response = s3.get_object(Bucket='iefp-unemployment', Key='copy_test_train/random_forest_T180509.pkl')\n",
    "model = pickle.loads(response[\"Body\"].read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "def generate_combinations(n,k):\n",
    "    result = list()\n",
    "    for i in range(1, k + 1):\n",
    "        for bits in itertools.combinations(range(n), i):\n",
    "                s = [0] * n\n",
    "                for bit in bits:\n",
    "                    s[bit] = 1\n",
    "                result.append(s)\n",
    "\n",
    "    return pd.DataFrame(result)\n",
    "\n",
    "def get_user_recommendations(journey_series, model, max_interventions=3, number_results=10, index=1):\n",
    "    # Get interventions to permute over\n",
    "    interventions = [col for col in journey_series.axes[0] if col[0:2] == \"i_\"]\n",
    "    journey_series.drop(labels=interventions + [\"ttj\", \"ttj_sub_12\"], inplace=True)\n",
    "    \n",
    "    # Generate permutation dataframe\n",
    "    permutations = generate_combinations(len(interventions), max_interventions)\n",
    "    \n",
    "    # Create user dataframe to match permutation size and join with permutation df\n",
    "    predict_df = pd.DataFrame(np.tile(journey_series.values,\n",
    "                              len(permutations.index)).reshape(-1,len(journey_series.index)),\n",
    "                              columns=journey_series.index).join(permutations)\n",
    "    \n",
    "    # Predict for all permutations\n",
    "    probabilities = pd.DataFrame(model.predict_proba(predict_df))\n",
    "    probabilities.columns = [\"unsuccessful\", \"successful\"]\n",
    "\n",
    "    #Create final top_n dataframe\n",
    "    results_df = predict_df.join(probabilities)\n",
    "    \n",
    "    top_n = results_df.sort_values(by=['successful'], ascending=False).iloc[0:number_results,:]\n",
    "    top_n[\"journey_id\"] = index\n",
    "\n",
    "    return top_n\n",
    "\n",
    "def gen_top_recommendations_table(df, model):\n",
    "    # Prepare empty dataframe\n",
    "    recommendations_df = pd.DataFrame()\n",
    "    \n",
    "    # Iterate through each user and append the top 5 recommendations to an empty dataframe\n",
    "    for index, journey in df.iterrows():\n",
    "        user_recommendations = get_user_recommendations(journey, model, index=index)\n",
    "        recommendations_df = pd.concat([recommendations_df,user_recommendations])\n",
    "\n",
    "    return recommendations_df\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test intervention count\n",
    "\n",
    "small_test = df_test.tail(10)\n",
    "\n",
    "start = time.time()\n",
    "top_5_10 = gen_top_recommendations_table(small_test, model)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = small_test.join(top_5_10.groupby(\"journey_id\").sum().add_prefix('sum_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_set = df_train.append(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub_group(df, observation):\n",
    "    \n",
    "    journey = pd.DataFrame(observation).T\n",
    "    \n",
    "    journey[\"youngest\"] = (journey[\"d_age\"] >= 0.0) & (journey[\"d_age\"] < 0.2)\n",
    "    journey[\"young_adult\"] = (journey[\"d_age\"] >= 0.2) & (journey[\"d_age\"] < 0.3)\n",
    "    journey[\"adult\"] = (journey[\"d_age\"] >= 0.3) & (journey[\"d_age\"] < 0.40)\n",
    "    journey[\"middle_adult\"] = (journey[\"d_age\"] >= 0.4) & (journey[\"d_age\"] < 0.5)\n",
    "    journey[\"older_adult\"] = (journey[\"d_age\"] >= 0.5) & (journey[\"d_age\"] < 0.6)\n",
    "    journey[\"senior\"] = (journey[\"d_age\"] >= 0.6) & (journey[\"d_age\"] < 0.7)\n",
    "    journey[\"old_senior\"] = (journey[\"d_age\"] >= 0.7) & (journey[\"d_age\"] < 0.8)\n",
    "    journey[\"old\"] = (journey[\"d_age\"] >= 0.8)\n",
    "    \n",
    "    df[\"youngest\"] = (df[\"d_age\"] >= 0.0) & (df[\"d_age\"] < 0.2)\n",
    "    df[\"young_adult\"] = (df[\"d_age\"] >= 0.2) & (df[\"d_age\"] < 0.3)\n",
    "    df[\"adult\"] = (df[\"d_age\"] >= 0.3) & (df[\"d_age\"] < 0.40)\n",
    "    df[\"middle_adult\"] = (df[\"d_age\"] >= 0.4) & (df[\"d_age\"] < 0.5)\n",
    "    df[\"older_adult\"] = (df[\"d_age\"] >= 0.5) & (df[\"d_age\"] < 0.6)\n",
    "    df[\"senior\"] = (df[\"d_age\"] >= 0.6) & (df[\"d_age\"] < 0.7)\n",
    "    df[\"old_senior\"] = (df[\"d_age\"] >= 0.7) & (df[\"d_age\"] < 0.8)\n",
    "    df[\"old\"] = (df[\"d_age\"] >= 0.8)\n",
    "    \n",
    "    dems = [\"d_gender_M\", \"d_disabled\", \"d_subsidy\", \"d_rsi_True\",\n",
    "           \"youngest\", \"young_adult\", \"adult\", \"middle_adult\", \n",
    "            \"older_adult\", \"senior\", \"old_senior\", \"old\", \"d_nationality_other\",\n",
    "           \"d_school_qualification_2.0\", \"d_school_qualification_3.0\",\n",
    "            \"d_school_qualification_4.0\", \"d_school_qualification_5.0\",\n",
    "            \"d_school_qualification_6.0\", \"d_school_qualification_nan\"]\n",
    "    \n",
    "    sub_group = df.merge(journey[dems], on=dems, right_index=True, how=\"inner\")\n",
    "\n",
    "    return sub_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = get_sub_group(full_set, matrix.iloc[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interns = sub[sub[\"i_professional_internships\"] == 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interns['ttj_sub_12'].astype(int).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_recommendations(journey, full_dataset, recommendations):\n",
    "    sub_group = get_sub_group(full_dataset, journey)\n",
    "    mean_success_rate = []\n",
    "    for rec in recommendations:\n",
    "        took_rec = sub_group[sub_group[rec] == 1.0]\n",
    "        mean_success_rate.append(took_rec['ttj_sub_12'].astype(int).mean())\n",
    "    average = sum(mean_success_rate) / len(mean_success_rate)\n",
    "    \n",
    "    return average"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iefp",
   "language": "python",
   "name": "iefp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
